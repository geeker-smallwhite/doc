code 评测集可能更偏向于函数级，比如 HLE, cf、或者 livecodebench，或者 GPQA、HUMANEVAL 这种的，但是在实际环境上 需要的应该是 能贴近 code 业务的，比如，swebench、比如 gittaskbench 等等。二者主要区别在于：

1. 实际贴近业务的评测集：
code 评测集可能函数级的评测集，一个题目也就是一个简单的函数(不超过20行),3、5个测试用例就能把功能全部覆盖测试完,稍微复杂一点的函数（100-200行），就需要大几百个测试用例来验证了。
跨文件，跨函数，甚至是跨仓库的，需要非常复杂的测试体系来验证功能的完整性，涉及多种形式测试，比如接口测试，单元测试，集成测试，这个是非常复杂的，而且非常需要人力。
2. 验证环境：
函数级别的评测集，没有下游服务调用，没有很多复杂的依赖使用，只需要一个 编译器/解释器 就可以运行。
业务级别的评测集： 因为依赖大量下游服务，并且有非常复杂的依赖调用，所以需要专门的沙盒，并且沙盒还能访问下游服务（一般是线上业务，为了不影响，还需要创建一个虚拟的服务）。功能完整性验证都依赖这个环境，所以这个是非常必要的。
3. prompt 构建（context）：
需要足够的信息，让模型能理解你让他干什么，但是不能太多了，如果太多就变成代码翻译的评测集了，比如 code 的评测集了，同样信息也不能太少，否则根本无法达到预期的效果。


